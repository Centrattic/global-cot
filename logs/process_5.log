Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/lambda/nfs/riya-probing/global-cot/src/activation_extractor.py", line 421, in <module>
    main()
  File "/lambda/nfs/riya-probing/global-cot/src/activation_extractor.py", line 387, in main
    extractor = ActivationExtractor()
  File "/lambda/nfs/riya-probing/global-cot/src/activation_extractor.py", line 34, in __init__
    self.model, self.tokenizer = FastLanguageModel.from_pretrained(
  File "/lambda/nfs/riya-probing/global-cot/.venv/lib/python3.10/site-packages/unsloth/models/loader.py", line 387, in from_pretrained
    return FastModel.from_pretrained(
  File "/lambda/nfs/riya-probing/global-cot/.venv/lib/python3.10/site-packages/unsloth/models/loader.py", line 910, in from_pretrained
    model, tokenizer = FastBaseModel.from_pretrained(
  File "/lambda/nfs/riya-probing/global-cot/.venv/lib/python3.10/site-packages/unsloth/models/vision.py", line 542, in from_pretrained
    model = auto_model.from_pretrained(
  File "/lambda/nfs/riya-probing/global-cot/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
  File "/lambda/nfs/riya-probing/global-cot/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 288, in _wrapper
    return func(*args, **kwargs)
  File "/lambda/nfs/riya-probing/global-cot/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5179, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/lambda/nfs/riya-probing/global-cot/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5600, in _load_pretrained_model
    caching_allocator_warmup(model_to_load, expanded_device_map, hf_quantizer)
  File "/lambda/nfs/riya-probing/global-cot/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 6234, in caching_allocator_warmup
    _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.50 GiB. GPU 0 has a total capacity of 94.50 GiB of which 67.40 GiB is free. Process 7604 has 636.00 MiB memory in use. Process 7601 has 19.61 GiB memory in use. Process 7600 has 636.00 MiB memory in use. Process 7599 has 636.00 MiB memory in use. Process 7602 has 636.00 MiB memory in use. Process 7603 has 636.00 MiB memory in use. Including non-PyTorch memory, this process has 636.00 MiB memory in use. Process 8070 has 636.00 MiB memory in use. Process 8072 has 816.00 MiB memory in use. Process 8071 has 636.00 MiB memory in use. Process 8069 has 956.00 MiB memory in use. Of the allocated memory 2.00 MiB is allocated by PyTorch, and 18.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environmentðŸ¦¥ UnslothðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Loading model with Unsloth FastLanguageModel
==((====))==  Unsloth 2025.9.11: Fast Gpt_Oss patching. Transformers: 4.56.2.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 94.5 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu129. CUDA: 9.0. CUDA Toolkit: 12.9. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
